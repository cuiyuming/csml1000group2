---
title: "Example"
author: "Yuming Cui"
date: "22/09/2019"
output: html_document
---


# Introduction
Melbourne is currently experiencing a housing bubble (some experts say it may burst soon). Maybe someone can find a trend or give a prediction? Which suburbs are the best to buy in? Which ones are value for money? Where's the expensive side of town? And more importantly where should I buy a 2 bedroom unit?

Some Key Details:

Suburb: Suburb

Address: Address

Rooms: Number of rooms

Price: Price in Australian dollars

Method: S - property sold; SP - property sold prior; PI - property passed in; PN - sold prior not disclosed; SN - sold not disclosed; NB - no bid; VB - vendor bid; W - withdrawn prior to auction; SA - sold after auction; SS - sold after auction price not disclosed. N/A - price or highest bid not available.

Type: br - bedroom(s); h - house,cottage,villa, semi,terrace; u - unit, duplex; t - townhouse; dev site - development site; o res - other residential.

SellerG: Real Estate Agent

Date: Date sold

Distance: Distance from CBD in Kilometres

Regionname: General Region (West, North West, North, North east ...etc)

Propertycount: Number of properties that exist in the suburb.

Bedroom2 : Scraped # of Bedrooms (from different source)

Bathroom: Number of Bathrooms

Car: Number of carspots

Landsize: Land Size in Metres

BuildingArea: Building Size in Metres

YearBuilt: Year the house was built

CouncilArea: Governing council for the area

Lattitude: Self explanitory

Longtitude: Self explanitory
There are three parts to my script as follows:

* Feature engineering
* Missing value imputation
* Prediction!

```{r}
knitr::opts_chunk$set(echo = TRUE)
```

```{r echo=F, message=FALSE, warning=FALSE, include=FALSE}
# Load packages
library('ggplot2') # visualization
library('ggthemes') # visualizsation
library('scales') # visualization
library('dplyr') # data manipulation
library('mice') # imputation
library(mlbench)
library(ggplot2)
library(lars)
library(RColorBrewer)
library(reshape2)
library(ggplot2)
library(e1071)
library(dplyr)
library(Amelia)
library(RANN)
library(arm)
library(caret)
library(ipred)
library(corrplot)
library(knitr)
library(Metrics)

# Read data and clear data.
raw <- read.csv('./data/raw/Melbourne_housing_FULL.csv',stringsAsFactors = F)

# We only look at Method: S - property sold; SP - property sold prior
clean <- subset(raw, (Method=="S" | Method=="SP") & (Price != "") )

# use the remaining 80% of data to training and testing the models
dataset <- clean

dataset$Distance <- as.numeric(as.character(dataset$Distance))
                               
dataset$Propertycount <- as.numeric(as.character(dataset$Propertycount))

```

# Corelation Graph
```{r}
library("ggpubr")
ggscatter(dataset, x = "Rooms", y = "Price", 
          add = "reg.line", conf.int = TRUE, 
          cor.coef = TRUE, cor.method = "pearson",
          xlab = "Number of Rooms", ylab = "Dollars")
```

#Correlation Matrix
```{r}
naomitData = na.omit(dataset)
numeric_cols = sapply(naomitData, is.numeric)
data_num_only=naomitData[,numeric_cols]
head(data_num_only)

data_num_subset = naomitData[ , c("Rooms", "Price", "Distance", "Bathroom", "Car", "Landsize", "BuildingArea", "Propertycount")]
head(data_num_subset)
cor(data_num_subset)
```


# Find the price outliers
```{r}

boxplot(naomitData$Price)


# Now you can assign the outlier values into a vector

outliers <- boxplot(naomitData$Price, plot=FALSE)$out

# Check the results

print(outliers)
```

# Remove the price outliers
```{r}
naomitData[which(naomitData$Price %in% outliers),]

# Now you can remove the rows containing the outliers, one possible option is:

naomitData <- naomitData[-which(naomitData$Price %in% outliers),]

# If you check now with boxplot, you will notice that those pesky outliers are gone

boxplot(naomitData$Price)
```

## MODEL AND MODEL DEVELOPMENT

```{r, echo=F, warning=F, message=F}
#Creating a base Linear Model using all the predictors.
lm_all <- standardize(
  lm(
    Price ~ Rooms +  Distance +  BuildingArea + Bathroom + Car      
    , data = naomitData
  )
)
summary(lm_all)
```
```{r}
lm_mod5 <- lm(
    log(Price) ~ Rooms +  Distance +  BuildingArea       
    
    , data = naomitData
  )
summary(standardize(lm_mod5))
cat("RMSE of the final model", rmse(naomitData$Price, exp(predict(lm_mod5))))  
```
# Divide the Train Dataset to test and train sets, for predicting the RMSE and R2

```{r}
library(Metrics)
train_part <- sample(nrow(naomitData), 0.7*nrow(naomitData), replace = F)
set.seed(77)
new_test <- naomitData[-train_part, ]
new_prediction <- predict(lm_mod5, newdata= new_test)
cat("RMSE on Test model", rmse(new_test$Price, exp(new_prediction)))  
R2 <- function(y, yhat, ybar, digits = 2) {
  1 - sum((y - yhat)^2)/sum((y - ybar)^2)
}
actualR2<-R2(y = new_test$Price, yhat = exp(predict(lm_mod5,newdata = new_test)), 
             mean(new_test$Price))
round(actualR2,2)
# Performing Cross Validation on Training Set
lm_model_cv <-train( log(Price) ~ Rooms +  Distance +  BuildingArea + Bathroom,
                     data = naomitData,
                     method= "lm",
                     trControl= trainControl(method="cv", number=10))
cat("RMSE: ", rmse(naomitData$Price, exp(predict(lm_model_cv, newdata=  naomitData))))
lm_model_cv$results$Rsquared
```
```