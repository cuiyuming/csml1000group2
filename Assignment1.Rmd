---
title: "Melbourne"
author: "Rajiv Kaushik, Madan Bolla, Pratik Chandwni, Yuming Cui"
date: "22/09/2019"
output: html_document
---


# Introduction
Melbourne is currently experiencing a housing bubble (some experts say it may burst soon). Maybe someone can find a trend or give a prediction? Which suburbs are the best to buy in? Which ones are value for money? Where's the expensive side of town? And more importantly where should I buy a 2 bedroom unit?

Some Key Details:

Suburb: Suburb

Address: Address

Rooms: Number of rooms

Price: Price in Australian dollars

Method: S - property sold; SP - property sold prior; PI - property passed in; PN - sold prior not disclosed; SN - sold not disclosed; NB - no bid; VB - vendor bid; W - withdrawn prior to auction; SA - sold after auction; SS - sold after auction price not disclosed. N/A - price or highest bid not available.

Type: br - bedroom(s); h - house,cottage,villa, semi,terrace; u - unit, duplex; t - townhouse; dev site - development site; o res - other residential.

SellerG: Real Estate Agent

Date: Date sold

Distance: Distance from CBD in Kilometres

Regionname: General Region (West, North West, North, North east ...etc)

Propertycount: Number of properties that exist in the suburb.

Bedroom2 : Scraped # of Bedrooms (from different source)

Bathroom: Number of Bathrooms

Car: Number of carspots

Landsize: Land Size in Metres

BuildingArea: Building Size in Metres

YearBuilt: Year the house was built

CouncilArea: Governing council for the area

Lattitude: Self explanitory

Longtitude: Self explanitory
There are three parts to my script as follows:

* Feature engineering
* Missing value imputation
* Prediction!

```{r}
knitr::opts_chunk$set(echo = TRUE)
```

```{r echo=F, message=FALSE, warning=FALSE}
# Load packages
library('ggplot2') # visualization
library('ggthemes') # visualizsation
library('scales') # visualization
library('dplyr') # data manipulation
library('mice') # imputation
library(mlbench)
library(ggplot2)
library(lars)
library(RColorBrewer)
library(reshape2)
library(ggplot2)
library(e1071)
library(dplyr)
library(Amelia)
library(RANN)
library(arm)
library(caret)
library(ipred)
library(corrplot)
library(knitr)
library(Metrics)
library(tidyverse)
library("ggpubr")
library(Hmisc)
library(DMwR)
```

# global functions######################
```{r echo=F, message=FALSE, warning=FALSE, include=FALSE}
#function to find columns that have NA in them
nacols <- function(x){
  y <- sapply(x, function(xx)any(is.na(xx)))
  names(y[y])
}


```

# import data and preprocessing of data######################

```{r echo=F, message=FALSE, warning=FALSE, include=FALSE}

# Read data and clear data.
dataset <- read_csv("./data/raw/Melbourne_housing_FULL.csv",col_types = cols(Price="i",Postcode="i",Landsize="i",
                                                              Rooms = "i", Bedroom2 = "i",Bathroom="i",Car="i",YearBuilt="i",Propertycount="i"))

# We only look at certain Methods
#Method: S - property sold; SP - property sold prior; PI - property passed in; PN - sold prior not disclosed; SN - sold not disclosed; NB - no bid; VB - vendor bid; W - withddatasetn prior to auction; SA - sold after auction; SS - sold after auction price not disclosed. N/A - price or highest bid not available.
dataset <- dataset %>% filter(Method %in% c("S", "SP","PN","SN","SA","SS"))

#funtion to clean up various types of illegit data on Price
dataset <- dataset %>%
  mutate(Price = replace(Price, Price == "na", NA)) %>%
  mutate(Price = replace(Price, Price == "N/A", NA)) %>%
  mutate(Price = replace(Price, Price == "", NA))
dataset <- dataset %>%
  mutate(Distance = replace(Distance, Distance == "na", NA)) %>%
  mutate(Distance = replace(Distance, Distance == "N/A", NA)) %>%
  mutate(Distance = replace(Distance, Distance == "", NA))
dataset <- dataset %>%
  mutate(Postcode = replace(Postcode, Postcode == "na", NA)) %>%
  mutate(Postcode = replace(Postcode, Postcode == "N/A", NA)) %>%
  mutate(Postcode = replace(Postcode, Postcode == "", NA))

#remove rows with no price
dataset <- dataset %>% filter(!is.na(Price))

#add rowid so we can get to specific row easily for cleaning
dataset <- tibble::rowid_to_column(dataset, "ROWID")

glimpse(dataset)

```


#Distribution of numeric variables
```{r echo=F, message=FALSE, warning=FALSE}

numericcols=c("ROWID","Rooms","Price","Distance","Postcode","Bedroom2","Bathroom","Car","Landsize","BuildingArea","YearBuilt","Propertycount")
numcols=dataset[numericcols]

#Melt data --> Turn it into skinny LONG table
melt_data = melt(numcols, id.vars=c("ROWID"))

#This data structure is now suitable for a multiplot function
ggplot(data = melt_data, mapping = aes(x = value)) + geom_histogram(bins = 10) + facet_wrap(~variable, scales = 'free_x')


```


### Data exploration, density of sales by sale price ######################

```{r echo=F, message=FALSE, warning=FALSE}
#subset to data which is complete or has no NAs
datasetcomplete <- dataset %>% filter(complete.cases(.))

# apply log to distribute it normally
hist(datasetcomplete$Price,main = "Sale Price Distribution",
     xlab = 'Sale Price',freq = FALSE,col=brewer.pal(8,"Set3"),las = 3,breaks = 190)
lines(density(datasetcomplete$Price))

```

### Data exploration, correlation matrix to better inform cleaning and imputation of  data later######################

```{r echo=F, message=FALSE, warning=FALSE}
selectcols=c("Rooms","Price","Distance","Postcode","Bedroom2","Bathroom","Car","Landsize","BuildingArea","YearBuilt","Propertycount")
selectdata=datasetcomplete[selectcols]

cor(selectdata)

```


### Data exploration, correlation graph to better inform cleaning and imputation of  data later######################
#we infer from this matrix 2 key facts
#1.that Price is correlated to 6 variables
#2. that Rooms and Bedrooms2 are stringly correlated. Implies that we will remove one of them from dataset
```{r echo=F, message=FALSE, warning=FALSE}


#correlation matrix with statistical significance
cor_result=rcorr(as.matrix(selectdata))


# flattenCorrMatrix - makes it easier to read
flattenCorrMatrix <- function(cormat, pmat) {
  ut <- upper.tri(cormat)
  data.frame(
    row = rownames(cormat)[row(cormat)[ut]],
    column = rownames(cormat)[col(cormat)[ut]],
    cor  =(cormat)[ut],
    p = pmat[ut]
  )
}

#flatten matrix
cor_result_flat = flattenCorrMatrix(cor_result$r, cor_result$P)


#display only the upper triangular of the correlation matrix.
corrplot(cor_result$r, type = "upper", order = "hclust", tl.col = "black", tl.srt = 45)

```



# scatter plot
#shows how sales and sale price is correlated to number of rooms 
```{r echo=F, message=FALSE, warning=FALSE}
ggscatter(dataset, x = "Rooms", y = "Price", 
          add = "reg.line", conf.int = TRUE, 
          cor.coef = TRUE, cor.method = "pearson",
          xlab = "Number of Rooms", ylab = "Price")
```



# show the price outliers before removing outliers
```{r echo=F, message=FALSE, warning=FALSE}

Q1 = quantile(dataset$Price, 0.25, na.rm=TRUE)
Q3 = quantile(dataset$Price, 0.75, na.rm=TRUE)


IQR <- IQR(dataset$Price, na.rm=TRUE)
#now to specify upper/lower limits
UL <- unname(Q3 + 1.5*IQR)
LL<- unname(max(Q1 - 1.5*IQR,0))
#LL <- 0


#need a better scale
boxplot(dataset$Price, las = 1, ylim=c(0,UL+.5*IQR))

```

### Data cleaning, remove outliers######################

```{r echo=F, message=FALSE, warning=FALSE, include=FALSE}
#remove outliers 
remove_outliers <- function(x) {
  qnt <- quantile(x, probs=c(.25, .75), na.rm = TRUE)
  H <- 1.5 * IQR(x, na.rm = TRUE)
  y=x
  y[x < (qnt[1] - H)] = -999999
  y[x > (qnt[2] + H)] = -999999
  y
}

#remove outliers and check after...
dataset$Price = remove_outliers(dataset$Price)



#now actually remove it
dataset <- dataset[rowSums(dataset ==-999999)==0, , drop = FALSE]

```

# show the price outliers AFTER removing outliers
```{r echo=F, message=FALSE, warning=FALSE}

Q1 = quantile(dataset$Price, 0.25, na.rm=TRUE)
Q3 = quantile(dataset$Price, 0.75, na.rm=TRUE)


IQR <- IQR(dataset$Price, na.rm=TRUE)
#now to specify upper/lower limits
UL <- unname(Q3 + 1.5*IQR)
LL<- unname(max(Q1 - 1.5*IQR,0))
#LL <- 0


#need a better scale
boxplot(dataset$Price, las = 1, ylim=c(0,UL+.5*IQR))

```

###### Impute data######################


```{r echo=F, message=FALSE, warning=FALSE}

#numeric variables
knncolnames=c("Rooms", "Distance", "Postcode", "Bedroom2", "Bathroom", "Car", "Landsize", "BuildingArea", "YearBuilt", "Propertycount")
knninput=as.data.frame(dataset[, names(dataset) %in% knncolnames])

#need to convert data type to data frame for knnimputation to work on it
knninput=as.data.frame(knninput)

#before starting impute show variables that have NA
print("Columns with NA values are as follows-")
nacols(knninput)

#Run KNN imputation for this sample - warning might take a while....
knnOutput = knnImputation(knninput, k=3, meth='median')

#"Rooms","Distance","Postcode","Bedroom2","Bathroom","Car","Landsize","BuildingArea","YearBuilt","Propertycount"
dataset$Rooms = knnOutput$Rooms
dataset$Distance =knnOutput$Distance
dataset$Postcode =knnOutput$Postcode 
dataset$Bedroom2 =knnOutput$Bedroom2 
dataset$Bathroom =knnOutput$Bathroom 
dataset$Car =knnOutput$Car 
dataset$Landsize =knnOutput$Landsize 
dataset$BuildingArea =knnOutput$BuildingArea 
dataset$YearBuilt =knnOutput$YearBuilt 
dataset$Propertycount =knnOutput$Propertycount 

#now find variables that have NA
print("Columns with NA values are as follows-")
nacols(knnOutput)

glimpse(knninput)
glimpse(knnOutput)


```


### Data Cleaning, remove highly correlatd variables######################


```{r echo=F, message=FALSE, warning=FALSE, include=FALSE}

#seems like bedroom and room are highly correlated, so remove one
#done only after imputation
dataset <- dataset  %>% dplyr::select(-c("Bedroom2"))

```

###### Split data for training and testing ######################


```{r echo=F, message=FALSE, warning=FALSE}


#split training and test data
set.seed(3456)
trainingIndex = createDataPartition(dataset$Regionname, p = 0.6, list=FALSE)
trainData = dataset[trainingIndex,]
testData = dataset[-trainingIndex,]
nrow(trainData)
```






### use trainData and testData for rest of work now



#
```{r}
library(dplyr)
print(getwd())
saveRDS(trainData, file = "./data/clean/traindata.Rds")
saveRDS(testData, file = "./data/clean/testdata.Rds")

```


## MODEL AND MODEL DEVELOPMENT

```{r, echo=F, warning=F, message=F}
#Creating a base Linear Model using all the predictors.
lm_all <- standardize(
  lm(
    Price ~ Rooms +  Distance +  BuildingArea + Bathroom + Car      
    , data = trainData
  )
)
summary(lm_all)
```
```{r}
lm_mod5 <- lm(
    log(Price) ~ Rooms +  Distance +  BuildingArea       
    
    , data = trainData
  )
summary(standardize(lm_mod5))
cat("RMSE of the final model", rmse(trainData$Price, exp(predict(lm_mod5))))  
```


# predicting RMSE and R2

```{r}

glimpse(testData)

library(Metrics)
set.seed(77)
new_prediction <- predict(lm_mod5, newdata= testData)
cat("RMSE on Test model", rmse(testData$Price, exp(new_prediction)))  
R2 <- function(y, yhat, ybar, digits = 2) {
  1 - sum((y - yhat)^2)/sum((y - ybar)^2)
}
actualR2<-R2(y = testData$Price, yhat = exp(predict(lm_mod5,newdata = testData)), 
             mean(testData$Price))
round(actualR2,2)
# Performing Cross Validation on Training Set
lm_model_cv <-train( log(Price) ~ Rooms +  Distance +  BuildingArea + Bathroom,
                     data = trainData,
                     method= "lm",
                     trControl= trainControl(method="cv", number=10))
cat("RMSE: ", rmse(trainData$Price, exp(predict(lm_model_cv, newdata=  trainData))))
lm_model_cv$results$Rsquared
```
